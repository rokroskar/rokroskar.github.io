<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Blog</title><link href="http://rokroskar.github.io/" rel="alternate"></link><link href="http://rokroskar.github.io/feeds/spark.atom.xml" rel="self"></link><id>http://rokroskar.github.io/</id><updated>2015-12-16T00:00:00+01:00</updated><entry><title>Monitoring Spark on Hadoop with Prometheus and Grafana</title><link href="http://rokroskar.github.io/monitoring-spark-on-hadoop-with-prometheus-and-grafana.html" rel="alternate"></link><updated>2015-12-16T00:00:00+01:00</updated><author><name>Rok</name></author><id>tag:rokroskar.github.io,2015-12-16:monitoring-spark-on-hadoop-with-prometheus-and-grafana.html</id><summary type="html">&lt;p&gt;Anyone who has spent time developing &lt;a href="http://spark.apache.org"&gt;Spark&lt;/a&gt; applications (or any other distributed application for that matter) has probably wished for some x-ray goggles into the black-box machinery of the framework. While Spark provides a nice and increasingly feature-rich UI for checking on the status of running tasks and even gives statistics on things like runtime, memory usage, disk I/O etc., there are other aspects of the runtime that can remain an annoying mystery: how is the JVM memory being utilized? How much memory is the driver using? What about garbage collection? As it turns out, all these are reported by &lt;a href="http://spark.apache.org/docs/latest/monitoring.html#metrics"&gt;Spark's metrics system&lt;/a&gt;: &lt;em&gt;they're out there, you just need to grab them&lt;/em&gt;.&lt;/p&gt;
&lt;div class="alert alert-warning"&gt;
&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Gaining insight into your Spark applications by collecting Spark metrics with tools like &lt;a href=http://prometheus.io&gt;Prometheus&lt;/a&gt; is &lt;emph&gt;easy&lt;/emph&gt; and can be done by &lt;emph&gt;anyone&lt;/emph&gt; with or without admin priviledges. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Unfortunately, the documentation regarding the metrics system is rather poor. If you also want to combine the Spark-reported metrics with those generated by Hadoop (YARN, HDFS), then you really embark on another google-powered goose chase for insights drawing on incomplete documentation pages and outdated blogs. I was inspired in this goose-chase by an excellent &lt;a href="http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/"&gt;blog post&lt;/a&gt; showing a nice use of Spark metrics (the only one I could find, actually) and set off to do this for my own system. (there is another nice post about &lt;a href="http://argus-sec.com/blog/monitoring-spark-prometheus/"&gt;using Prometheus to monitor Spark Streaming&lt;/a&gt;, but using the JMX exporter instead of Graphite)&lt;/p&gt;
&lt;h2&gt;Goals&lt;/h2&gt;
&lt;p&gt;My main goals were two-fold: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use metrics to better understand the JVM runtime of Spark applications&lt;/li&gt;
&lt;li&gt;combine spark, hadoop, and system-level metrics to complement performance benchmarks when making system architecture decisions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first is somewhat obvious - tired of mysterious "Out of memory" exceptions, I want more fine-grained information about where, when, and why  the problems arise. It is especially difficult to get any kind of information about off-heap memory usage and garbage collection by standard means in Spark and I want to rectify this situation. &lt;/p&gt;
&lt;p&gt;The second is slightly more complex - we are running a 250+ node "test" Spark/Hadoop cluster on somewhat outdated hardware that is being used a sandbox before we purchase a modern state-of-the-art machine. Benchmarks like Terasort on Hadoop or the &lt;a href="https://github.com/databricks/spark-perf"&gt;spark-perf&lt;/a&gt; test suite give you timing information but not very much data on what the system is actually doing. What are the raw disk I/O rates on individual nodes? Is the network being saturated? Is HDFS performance hampered by slow disks, network, cpu? When we run the same benchmark on a new system and get a (hopefully) improved time, which of these factors was most important and where could we perhaps downgrade components to save money without sacrificing performance? To answer these questions we really need instrumentation and monitoring. &lt;/p&gt;
&lt;h2&gt;Choices of monitoring backend and visualization&lt;/h2&gt;
&lt;h3&gt;Graphite&lt;/h3&gt;
&lt;p&gt;The widely-adopted general-purpose monitoring choice seems to be &lt;a href="http://graphite.readthedocs.org/en/1.0/overview.html"&gt;Graphite&lt;/a&gt;. I found it pretty difficult to set up, owing to inconsistent documentation (for example, the top google hit for &lt;a href="https://www.google.ch/webhp?sourceid=chrome-instant&amp;amp;ion=1&amp;amp;espv=2&amp;amp;ie=UTF-8#q=graphite%20monitoring"&gt;"graphite monitoring"&lt;/a&gt; takes you to outdated docs) and many components that need to play nice together. I spent a day configuring graphite/carbon and had a working system after some headache. When I needed to add Grafana on top of this, I quickly reached for a &lt;a href="https://github.com/pellepelster/graphite-grafana-vagrant-box"&gt;Vagrant VM setup&lt;/a&gt; that worked very well, but I didn't want to rely on a Vagrant image when I actually tried to deploy this later. &lt;/p&gt;
&lt;p&gt;In addition, the built-in Graphite UI is fairly basic at best. The plotting is rather cumbersome and outdated, though I'm sure it's possible to set up nice dashboards with some effort. Still, it was very useful as an initial metrics browser, just to get a feeling for what is being reported. &lt;/p&gt;
&lt;h3&gt;Prometheus&lt;/h3&gt;
&lt;p&gt;A colleague pointed me to &lt;a href="http://prometheus.io/"&gt;Prometheus&lt;/a&gt; which on the other hand took me about five seconds to get running. No database/apache configurations needed. Just &lt;a href="https://github.com/prometheus/prometheus/releases"&gt;download the appropriate release&lt;/a&gt; and go. Alternatively, you can &lt;a href="http://prometheus.io/docs/introduction/install/#using-docker"&gt;run it easily via docker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As an added bonus, I liked a few features of Prometheus that I hadn't really thought about before trying Graphite:&lt;/p&gt;
&lt;h4&gt;The data model&lt;/h4&gt;
&lt;p&gt;The data model allows you to define metrics which are more like "metric containers" and give them fine-grained specifications using "labels". In essense, the labels are the "dimensions" of each metric. For example, your metric might be "latency" and your labels would be "hostname" and "operating_system". You can then easily look at aggregate statistics on "latency" or drill down seamlessly to get stats per host or per os. Pretty nice. &lt;/p&gt;
&lt;h4&gt;The Query Language&lt;/h4&gt;
&lt;p&gt;This is intimately tied to the data model, but Prometheus comes with a pretty nice query language. Of course you have to learn a few things about the syntax, but once you do it's pretty easy to use and has some nice features that allow you to take advantage of the multi-dimensionality of the metrics. &lt;/p&gt;
&lt;h4&gt;Scraping vs. pushing metrics&lt;/h4&gt;
&lt;p&gt;With Prometheus you have to define endpoints that it will "scrape" -- it doesn't get any data automatically and clients can't push data to it. This is nice if you want some control over potentially noisy sources. You don't have to alter the source, you can just stop scraping it for input temporarily. &lt;/p&gt;
&lt;h3&gt;Grafana&lt;/h3&gt;
&lt;p&gt;I haven't experimented very much with the visualization front-end but went straight for &lt;a href="http://grafana.org"&gt;Grafana&lt;/a&gt;. It was designed to be used with Graphite, but it is now possible to seamlessly insert Prometheus as a data source. Grafana looks good, has nice functionality, and seems fairly general so it seemed like a pretty safe choice. &lt;/p&gt;
&lt;h2&gt;Connecting Spark with Prometheus&lt;/h2&gt;
&lt;p class="alert alert-info"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Before you continue here, make sure your Prometheus instance is running and you can reach it at &lt;a href="http://localhost:9090"&gt;http://localhost:9090&lt;/a&gt; or whatever other port you configured.&lt;/p&gt;

&lt;p&gt;Spark doesn't have Prometheus as one of the pre-packaged sinks - so the strategy here is to ask Spark to export Graphite metrics and feed those into Prometheus via an exporter plugin. To report metrics to Graphite, you must set up metrics via a &lt;code&gt;metrics.properties&lt;/code&gt; file. You can put this in &lt;code&gt;$SPARK_HOME/config&lt;/code&gt; or pass it to spark on the command line by using  &lt;code&gt;--conf spark.metrics.conf=/path/to/metrics/file&lt;/code&gt; - beware that this path must either exist on all executors. Alternatively you can pass the file to the executors using the &lt;code&gt;--file&lt;/code&gt; flag. &lt;/p&gt;
&lt;p&gt;My &lt;code&gt;metrics.properties&lt;/code&gt; looks like this: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink
*.sink.graphite.host=&amp;lt;metrics_hostname&amp;gt;
*.sink.graphite.port=&amp;lt;metrics_port&amp;gt;
*.sink.graphite.period=5
*.sink.graphite.unit=seconds

# Enable jvm source for instance master, worker, driver and executor
master.source.jvm.class=org.apache.spark.metrics.source.JvmSource

worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource

driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource

executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Spark's monitoring sinks include Graphite, but not Prometheus. Luckily it's really easy to get Graphite data into Prometheus using the &lt;a href="https://github.com/prometheus/graphite_exporter"&gt;Graphite Exporter&lt;/a&gt;, which you can easily get running either by building from source or using the Docker image. Once it's up, all you need to do is change the port to which your Graphite clients (i.e. Spark in this case) are sending their metrics and you're set -- the default port is 9109 so make sure you set that in your &lt;code&gt;metrics.properties&lt;/code&gt; file. &lt;/p&gt;
&lt;p&gt;You can go to &lt;a href="http://localhost:9108/metrics"&gt;http://localhost:9108/metrics&lt;/a&gt; once the exporter is running to see which metrics it has collected - initially it will only have some internal metrics. To get spark metrics in there, make sure you set up the &lt;code&gt;metrics.properties&lt;/code&gt; file and try running the spark pi example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nv"&gt;$SPARK_HOME&lt;/span&gt;/bin/spark-submit  --master local&lt;span class="o"&gt;[&lt;/span&gt;*&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="nv"&gt;$SPARK_HOME&lt;/span&gt;/examples/src/main/python/pi.py 500
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On the http://localhost:9108/metrics you should now see a ton of lines like this: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# HELP local_driver_jvm_heap_init Graphite metric local.driver.jvm.heap.init
# TYPE local_driver_jvm_heap_init gauge
local_driver_jvm_heap_init 1.073741824e+09
# HELP local_driver_jvm_heap_max Graphite metric local-1450274266632.driver.jvm.heap.max
# TYPE local_driver_jvm_heap_max gauge
local_driver_jvm_heap_max 1.029177344e+09
# HELP local_driver_jvm_heap_usage Graphite metric local-1450274266632.driver.jvm.heap.usage
# TYPE local_driver_jvm_heap_usage gauge
local_driver_jvm_heap_usage 0.35
# HELP local_driver_jvm_heap_used Graphite metric local-1450274266632.driver.jvm.heap.used
# TYPE local_driver_jvm_heap_used gauge
local_driver_jvm_heap_used 3.60397752e+08
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is showing us that the Graphite exporter to Prometheus works, but by default all Graphite metrics are sent across just as 1D metrics to Prometheus, i.e. without any label dimensions. To get the data into the Prometheus data model, we have to set up a mapping. &lt;/p&gt;
&lt;h3&gt;Mapping Spark's Graphite metrics to Prometheus&lt;/h3&gt;
&lt;p&gt;The one trick here is that if you just send raw Graphite metrics to Prometheus, you will not be able to use the nice Prometheus query language to its fullest because the metrics data will not have labels.&lt;/p&gt;
&lt;p&gt;You can easily define mappings to turn these into proper Prometheus labeled metrics by specifying a mapping config file. Turning these JVM memory metrics into Prometheus metrics can be done with something like this: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*.*.jvm.*.*
name=&amp;quot;jvm_memory_usage&amp;quot;
application=&amp;quot;$1&amp;quot;
executor_id=&amp;quot;$2&amp;quot;
mem_type=&amp;quot;$3&amp;quot;
qty=&amp;quot;$4&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This instructs the exporter to create a metric named &lt;code&gt;jvm_memory_usage&lt;/code&gt; with labels &lt;code&gt;application&lt;/code&gt;, &lt;code&gt;executor_id&lt;/code&gt;, &lt;code&gt;mem_type&lt;/code&gt;, and &lt;code&gt;qty&lt;/code&gt;. After we restart the exporter with &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host:~/graphite_exporter rok$ ./graphite_exporter -graphite.mapping-config graphite_exporter_mapping
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and rerun the spark pi example, the metrics now look like this: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jvm_memory_usage{application=&amp;quot;application_ID&amp;quot;,executor_id=&amp;quot;1&amp;quot;,mem_type=&amp;quot;non-heap&amp;quot;,qty=&amp;quot;committed&amp;quot;} 3.76832e+07
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Great, now we can actually use Prometheus queries on our data!&lt;/p&gt;
&lt;p&gt;Here is my full graphite exporter mappings file that will turn Spark Graphite metrics into something usable in Prometheus: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*.*.executor.filesystem.*.*
name=&amp;quot;filesystem_usage&amp;quot;
application=&amp;quot;$1&amp;quot;
executor_id=&amp;quot;$2&amp;quot;
fs_type=&amp;quot;$3&amp;quot;
qty=&amp;quot;$4&amp;quot;

*.*.jvm.*.*
name=&amp;quot;jvm_memory_usage&amp;quot;
application=&amp;quot;$1&amp;quot;
executor_id=&amp;quot;$2&amp;quot;
mem_type=&amp;quot;$3&amp;quot;
qty=&amp;quot;$4&amp;quot;

*.*.jvm.pools.*.*
name=&amp;quot;jvm_memory_pools&amp;quot;
application=&amp;quot;$1&amp;quot;
executor_id=&amp;quot;$2&amp;quot;
mem_type=&amp;quot;$3&amp;quot;
qty=&amp;quot;$4&amp;quot;

*.*.executor.threadpool.*
name=&amp;quot;executor_tasks&amp;quot;
application=&amp;quot;$1&amp;quot;
executor_id=&amp;quot;$2&amp;quot;
qty=&amp;quot;$3&amp;quot;

*.*.BlockManager.*.*
name=&amp;quot;block_manager&amp;quot;
application=&amp;quot;$1&amp;quot;
executor_id=&amp;quot;$2&amp;quot;
type=&amp;quot;$3&amp;quot;
qty=&amp;quot;$4&amp;quot;

DAGScheduler.*.*
name=&amp;quot;DAG_scheduler&amp;quot;
type=&amp;quot;$1&amp;quot;
qty=&amp;quot;$2&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Exploring metrics in Prometheus&lt;/h2&gt;
&lt;p&gt;To actually see our Spark metrics in Prometheus, we need to tell it to scrape the graphite exporter for data. We do this by adding a job to &lt;code&gt;prometheus.yml&lt;/code&gt; below the internal &lt;code&gt;prometheus&lt;/code&gt; job declaration: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;...

scrape_configs:

...

  - job_name: &amp;#39;spark&amp;#39;

    target_groups:
      - targets: [&amp;#39;localhost:9108&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now restart Prometheus (if it was running already) and it should start collecting metrics from the exporter. Rerun the spark pi example to get some metrics collected. &lt;/p&gt;
&lt;p&gt;Prometheus comes with a simple web UI that should be accessible on &lt;a href="http://localhost:9090"&gt;http://localhost:9090&lt;/a&gt;. This allows you to try out some queries, for example you can enter this query: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jvm_memory_usage{executor_id=&amp;#39;driver&amp;#39;, qty=&amp;#39;used&amp;#39;, application=&amp;quot;local-1450275288942&amp;quot;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;but replace the application identifier with your actual application ID and see the values reported back in the "Console" tab or the plot. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Basic Prometheus plot" src="http://rokroskar.github.io/images/basic_prometheus.png" /&gt;&lt;/p&gt;
&lt;p&gt;This is nice to get a first look at your data, but for some sort of user-friendly metrics tracking, we'll want to set up &lt;a href="http://grafana.org"&gt;Grafana&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;Using Grafana to Visualize Spark metrics via Prometheus&lt;/h2&gt;
&lt;p&gt;First, &lt;a href="http://grafana.org/download/"&gt;download Grafana&lt;/a&gt; or use their &lt;a href="http://docs.grafana.org/installation/docker/"&gt;Docker container&lt;/a&gt;. I found that the build was breaking in ways I wasn't able to debug very quickly so I resorted to using the Docker container for the purposes of testing. &lt;/p&gt;
&lt;p&gt;Once Grafana is running, set up the Prometheus data source:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Prometheus data source" src="http://rokroskar.github.io/images/add_grafana_source.png" /&gt;&lt;/p&gt;
&lt;p&gt;Now you are ready to set up some Grafana dashboards! When adding plots, just select the "Prometheus" data source in the bottom right and enter a query. Here's an example: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Example of Prometheus plot in Grafana" src="http://rokroskar.github.io/images/example_grafana_prometheus_plot.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this example I'm using a template variable "application_ID" so that I can easily select the application I want. To define your own, go to the "templating" settings: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Templating in Grafana" src="http://rokroskar.github.io/images/grafana_templating.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Templating in Grafana" src="http://rokroskar.github.io/images/grafana_templating_detail.png" /&gt;&lt;/p&gt;
&lt;p&gt;See the &lt;a href="http://docs.grafana.org/datasources/prometheus/"&gt;Grafana Prometheus documentation&lt;/a&gt; for more information. &lt;/p&gt;
&lt;p&gt;Finally, a complete dashboard for a single Spark application showing some individual and aggregate Spark metrics may look like this: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Full Spark Grafana dashboard" src="http://rokroskar.github.io/images/grafana_full_spark_dashboard.png" /&gt;&lt;/p&gt;
&lt;p&gt;You can play with the full snapshot &lt;a href="https://snapshot.raintank.io/dashboard/snapshot/kHmB0PX9COdomGLCjA9LE3YhWCLVIXp5"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to use this dashboard as a template, you can &lt;a href="https://gist.github.com/rokroskar/1649be3c00bb802289eb"&gt;grab the JSON&lt;/a&gt; and import it in your own Grafana instance. &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is just the beginning of Spark/Prometheus/Grafana integration - next is obviously the collection of Hadoop and system-level metrics. More on this in the next post. &lt;/p&gt;</summary><category term="spark"></category><category term="prometheus"></category><category term="metrics"></category><category term="jvm"></category><category term="graphite"></category><category term="grafana"></category></entry></feed>